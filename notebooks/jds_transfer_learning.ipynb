{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impractable transfer learning tflearn example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib\n",
    "import os\n",
    "from   os      import getcwd\n",
    "from   os      import listdir\n",
    "from   os.path import isfile, join, isdir\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.metrics import Accuracy\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Fish Setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainPath      = '../data/raw/train'\n",
    "testPath       = '../data/raw/test_stg1'\n",
    "rawdataPath    = '../data/raw'\n",
    "fish_classes   = [f for f in listdir(trainPath) if isdir(join(trainPath, f))]\n",
    "NUM_CATEGORIES  = len(fish_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `build hdf5 image dataset`\n",
    "\n",
    "Set BUILD_HDF5_DATASET to True if have not created dataset yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 25s, sys: 937 ms, total: 2min 26s\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#If the training images differ in size to the image input layer of the pretrained network, \n",
    "# then you must resize or crop the image data\n",
    "BUILD_HDF5_DATASET = True\n",
    "IMAGE_SIZE         = 32\n",
    "VALIDATION_SPLIT   = True\n",
    "output_path        = join(rawdataPath, 'fish_dataset_{}x{}.h5'.format(IMAGE_SIZE, IMAGE_SIZE))\n",
    "input_path         = join(rawdataPath, 'train')\n",
    "\n",
    "if BUILD_HDF5_DATASET:\n",
    "    # Build a HDF5 dataset (only required once)\n",
    "    from tflearn.data_utils import build_hdf5_image_dataset\n",
    "\n",
    "\n",
    "    build_hdf5_image_dataset(target_path        =input_path, \n",
    "                             image_shape        =(IMAGE_SIZE, IMAGE_SIZE), \n",
    "                             mode               ='folder', \n",
    "                             output_path        =output_path, \n",
    "                             categorical_labels =True, \n",
    "                             normalize          =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `load fish X Y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 131 ms, sys: 87.5 ms, total: 218 ms\n",
      "Wall time: 253 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from   sklearn.model_selection    import train_test_split\n",
    "\n",
    "# Load HDF5 dataset\n",
    "import h5py\n",
    "\n",
    "h5f         = h5py.File(output_path, 'r')\n",
    "X_all       = h5f['X'][()]\n",
    "Y_all       = h5f['Y'][()]\n",
    "\n",
    "# Split into \n",
    "if VALIDATION_SPLIT:\n",
    "    X, X_valid, Y, Y_valid = train_test_split(X_all, Y_all, \n",
    "                                                          test_size    =0.2, \n",
    "                                                          random_state =23, \n",
    "                                                          stratify     =Y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Train a model on the CIFAR-10 dataset and save it` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data loading and organization\n",
    "from tflearn.datasets import cifar10\n",
    "\n",
    "(CX, CY), (CX_test, CY_test) = cifar10.load_data()\n",
    "CX, CY = shuffle(CX, CY)\n",
    "CY = to_categorical(CY, 10)\n",
    "CY_test = to_categorical(CY_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m1.27351\u001b[0m\u001b[0m | time: 19.335s\n",
      "| Adam | epoch: 001 | loss: 1.27351 - acc: 0.5454 -- iter: 49900/50000\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m1.27581\u001b[0m\u001b[0m | time: 20.781s\n",
      "| Adam | epoch: 001 | loss: 1.27581 - acc: 0.5478 | val_loss: 1.10584 - val_acc: 0.6080 -- iter: 50000/50000\n",
      "--\n",
      "INFO:tensorflow:/Users/stokesjd/repos/fish/sashimdig/notebooks/cifar10_cnn.tfl.ckpt-500 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:/Users/stokesjd/repos/fish/sashimdig/notebooks/cifar10_cnn.tfl is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# train and save a model\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "\n",
    "    # Real-time data preprocessing\n",
    "    img_prep = ImagePreprocessing()\n",
    "    img_prep.add_featurewise_zero_center()\n",
    "    img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "    # Real-time data augmentation\n",
    "    img_aug = ImageAugmentation()\n",
    "    img_aug.add_random_flip_leftright()\n",
    "    img_aug.add_random_rotation(max_angle=25.)\n",
    "\n",
    "    # Convolutional network building\n",
    "    network = input_data(shape=[None, 32, 32, 3],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "    network = conv_2d(network, 32, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = fully_connected(network, 512, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 10, activation='softmax')\n",
    "    network = regression(network, optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0,checkpoint_path='cifar10_cnn.tfl.ckpt')\n",
    "\n",
    "    # Train using classifier\n",
    "    model.fit(CX, CY, n_epoch=1, shuffle=True, validation_set=(CX_test, CY_test),\n",
    "              show_metric=True, batch_size=100, run_id='cifar10_cnn')\n",
    "\n",
    "    model.save(\"cifar10_cnn.tfl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 999  | total loss: \u001b[1m\u001b[32m1.02793\u001b[0m\u001b[0m | time: 19.073s\n",
      "| Adam | epoch: 001 | loss: 1.02793 - acc: 0.6336 -- iter: 49900/50000\n",
      "Training Step: 1000  | total loss: \u001b[1m\u001b[32m1.01495\u001b[0m\u001b[0m | time: 20.530s\n",
      "| Adam | epoch: 001 | loss: 1.01495 - acc: 0.6373 | val_loss: 0.93721 - val_acc: 0.6706 -- iter: 50000/50000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# load that trained model and rerun\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "\n",
    "    # Real-time data preprocessing\n",
    "    img_prep = ImagePreprocessing()\n",
    "    img_prep.add_featurewise_zero_center()\n",
    "    img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "    # Real-time data augmentation\n",
    "    img_aug = ImageAugmentation()\n",
    "    img_aug.add_random_flip_leftright()\n",
    "    img_aug.add_random_rotation(max_angle=25.)\n",
    "\n",
    "    # Convolutional network building\n",
    "    network = input_data(shape=[None, 32, 32, 3],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "    network = conv_2d(network, 32, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = fully_connected(network, 512, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 10, activation='softmax')\n",
    "    network = regression(network, optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    model_ref = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model_ref.load('cifar10_cnn.tfl')\n",
    "    # Train using classifier\n",
    "    model_ref.fit(CX, CY, n_epoch=1, shuffle=True, validation_set=(CX_test, CY_test),\n",
    "              show_metric=True, batch_size=100, run_id='cifar10_cnn_refine')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1104  | total loss: \u001b[1m\u001b[32m1.19064\u001b[0m\u001b[0m | time: 7.913s\n",
      "| Adam | epoch: 001 | loss: 1.19064 - acc: 0.5469 -- iter: 3020/3021\n",
      "Training Step: 1105  | total loss: \u001b[1m\u001b[32m1.31475\u001b[0m\u001b[0m | time: 9.250s\n",
      "| Adam | epoch: 001 | loss: 1.31475 - acc: 0.5322 | val_loss: 1.17302 - val_acc: 0.5728 -- iter: 3021/3021\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# now load that model and run on the fish data. choose to not load weights using restore=False\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "\n",
    "    # Real-time data preprocessing\n",
    "    img_prep = ImagePreprocessing()\n",
    "    img_prep.add_featurewise_zero_center()\n",
    "    img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "    # Real-time data augmentation\n",
    "    img_aug = ImageAugmentation()\n",
    "    img_aug.add_random_flip_leftright()\n",
    "    img_aug.add_random_rotation(max_angle=25.)\n",
    "\n",
    "    # Convolutional network building\n",
    "    network = input_data(shape=[None, IMAGE_SIZE, IMAGE_SIZE, 3],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "    network = conv_2d(network, 32, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = fully_connected(network, 512, activation='relu',restore=False)\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, NUM_CATEGORIES, activation='softmax',restore=False)\n",
    "    network = regression(network, optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    model_ref = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model_ref.load('cifar10_cnn.tfl')\n",
    "    # Train using classifier\n",
    "    model_ref.fit(X, Y, n_epoch=1, shuffle=True, validation_set=(X_valid, Y_valid),\n",
    "              show_metric=True, batch_size=5, run_id='fish_cnn_refine')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
