{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import tflearn packages\n",
    "\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tflearn.data_utils import shuffle\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "\n",
    "#import other packages\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import os\n",
    "from os import getcwd\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "\n",
    "import skimage\n",
    "from skimage import measure\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#randomly choose a subset of training images to make it run faster\n",
    "\n",
    "#import images\n",
    "\n",
    "def get_paths(foldNames):\n",
    "  \n",
    "    paths = dict.fromkeys(foldNames)\n",
    "\n",
    "    for idx,g in enumerate(foldNames):\n",
    "        fileNames = [f for f in listdir(join(trainPath,g)) if isfile(join(trainPath,g, f))]\n",
    "        for i,f in enumerate(fileNames):\n",
    "            fileNames[i] = join(trainPath,g,f)     \n",
    "        paths[g] = fileNames\n",
    "        \n",
    "    return paths\n",
    "\n",
    "trainPath = '../data/raw/train'\n",
    "testPath = '../data/raw/test_stg1'\n",
    "fish_classes = [f for f in listdir(trainPath) if isdir(join(trainPath, f))]\n",
    "groupData = pd.DataFrame ({'group': fish_classes})\n",
    "fish_paths = get_paths(fish_classes)\n",
    "\n",
    "keys_to_remove = [key for key, value in fish_paths.iteritems()\n",
    "                  if '.DS_Store' in value]\n",
    "\n",
    "for key in keys_to_remove:\n",
    "    del fish_paths[key]\n",
    "    \n",
    "#for key, item in fish_paths:\n",
    "#    if \".DS_Store\" in item:\n",
    "#       del fish_paths[key]\n",
    "        \n",
    "keys_to_remove        \n",
    "#this I did to print the paths to make sure I understand the dictionary\n",
    "#n = 1\n",
    "#for k,v in fish_paths.items():\n",
    "#    if '.DS_Store' in v:\n",
    "#        print(k, v)\n",
    "#    if n == 1:\n",
    "#        print(v)\n",
    "#        n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#label images by directory\n",
    "\n",
    "for idx,fish in enumerate(fish_classes):\n",
    "    groupData.ix[idx,'num files'] = int(len(fish_paths[fish]))\n",
    "    \n",
    "files = []\n",
    "Y_cat = []\n",
    "\n",
    "for fish in fish_classes:\n",
    "    fish_files = fish_paths[fish]\n",
    "    files.extend(fish_files)\n",
    "    \n",
    "    y_fish = np.tile(fish, len(fish_files))\n",
    "    Y_cat.extend(y_fish)\n",
    "\n",
    "\n",
    "is_to_remove = [i for i in range(0,len(files))\n",
    "                  if '.DS_Store' in files[i]]\n",
    "\n",
    "print(is_to_remove)\n",
    "\n",
    "for i in is_to_remove:\n",
    "    del files[i]\n",
    "    del Y_cat[i]\n",
    "        \n",
    "Y_cat = np.array(Y_cat) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#downsample images\n",
    "def read_image(src):\n",
    "    \"\"\"Read and resize individual images\"\"\"\n",
    "    im = io.imread(src)\n",
    "    im = resize(im, (ROWS, COLS))\n",
    "    return im\n",
    "\n",
    "ROWS = 9  #90 720\n",
    "COLS = 16 #160 1280\n",
    "CHANNELS = 3\n",
    "\n",
    "X_all = np.ndarray((len(files), ROWS, COLS, CHANNELS), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 3777\n",
      "Processed 1000 of 3777\n",
      "Processed 2000 of 3777\n",
      "Processed 3000 of 3777\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAAEoCAYAAAB8egmGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a17b710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, f in enumerate(files): \n",
    "    im = read_image(f)\n",
    "    X_all[i] = im\n",
    "    if i%1000 == 0: print('Processed {} of {}'.format(i, len(files)))\n",
    "\n",
    "##view example image\n",
    "image = X_all[0]\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(im, cmap='gray', interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split into train and test for cross-validation\n",
    "# One Hot Encoding Labels\n",
    "#    Transform the categorical array Y_all into matrix of the same height, \n",
    "#    but with a boolean column for each category.\n",
    "Y_all = LabelEncoder().fit_transform(Y_cat)\n",
    "Y_all = np_utils.to_categorical(Y_all)\n",
    "\n",
    "# test_size: between 0 and 1. proportion of the dataset to include in the test split\n",
    "# random_state: Pseudo-random number generator state used for random sampling. How to shoose this?\n",
    "# stratify: this is ensuring that the split datasets are balanced, i.e. contains the same \n",
    "# percentage of classes\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_all, Y_all, \n",
    "                                                    test_size=0.2, random_state=23, \n",
    "                                                    stratify=Y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build a simple CNN\n",
    "\n",
    "def dnn_test1():\n",
    "    #needed to run this tensorflow operation in order to build the network and subsequently \n",
    "    #create the model, multiple times. Rebuilding without resetting the tf.Graph object produces\n",
    "    #errors. Could also get around this issue by restarting kernel, but that's annoying.\n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        #input layer with shape of data specified. In this case, dimensions of our images, \n",
    "        #rows X cols X rgb array. The initial 'None' is for an unknown dimension reflecting the \n",
    "        #\"number of samples that are processed in a batch\"\n",
    "        \n",
    "        # Building convolutional network\n",
    "\n",
    "        net = input_data(shape=[None, ROWS, COLS, 3])\n",
    "        net = conv_2d(net, 32, 3, activation='relu', regularizer=\"L2\")\n",
    "        net = max_pool_2d(net, 2)\n",
    "        net = local_response_normalization(net)\n",
    "        net = conv_2d(net, 64, 3, activation='relu', regularizer=\"L2\")\n",
    "        net = max_pool_2d(net, 2)\n",
    "        net = local_response_normalization(net)\n",
    "        net = fully_connected(net, 72, activation='relu')\n",
    "        net = fully_connected(net, 8, activation='softmax')\n",
    "        net = regression(net)\n",
    "        return tflearn.DNN(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m1.60679\u001b[0m\u001b[0m | time: 3.489s\n",
      "| Adam | epoch: 001 | loss: 1.60679 - acc: 0.4630 -- iter: 3008/3021\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m1.58496\u001b[0m\u001b[0m | time: 3.506s\n",
      "| Adam | epoch: 001 | loss: 1.58496 - acc: 0.4729 -- iter: 3021/3021\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "#test on subset of training images you didn't use; if it works well could do a full train:test and submit\n",
    "# Define model\n",
    "model = dnn_test1()\n",
    "\n",
    "# Start training (apply gradient descent algorithm). Will want to specify multiple epochs \n",
    "# typically unless just testing\n",
    "model.fit(X_train, Y_train, n_epoch=1,\n",
    "          show_metric=True, batch_size=16)\n",
    "\n",
    "#started running at 12:22 PM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model predict\n",
    "# read in test photo set\n",
    "test_files = [im for im in os.listdir(testPath)]\n",
    "test = np.ndarray((len(test_files), ROWS, COLS, CHANNELS), dtype=np.uint8)\n",
    "for i, im in enumerate(test_files): \n",
    "    test[i] = read_image(join(testPath,im))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> img_00005.jpg</td>\n",
       "      <td> 0.367190</td>\n",
       "      <td> 0.072739</td>\n",
       "      <td> 0.040302</td>\n",
       "      <td> 0.023501</td>\n",
       "      <td> 0.130083</td>\n",
       "      <td> 0.096381</td>\n",
       "      <td> 0.053572</td>\n",
       "      <td> 0.216231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> img_00007.jpg</td>\n",
       "      <td> 0.465186</td>\n",
       "      <td> 0.056423</td>\n",
       "      <td> 0.022733</td>\n",
       "      <td> 0.012973</td>\n",
       "      <td> 0.128789</td>\n",
       "      <td> 0.083109</td>\n",
       "      <td> 0.037668</td>\n",
       "      <td> 0.193119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> img_00009.jpg</td>\n",
       "      <td> 0.441673</td>\n",
       "      <td> 0.061966</td>\n",
       "      <td> 0.026160</td>\n",
       "      <td> 0.014899</td>\n",
       "      <td> 0.127683</td>\n",
       "      <td> 0.086496</td>\n",
       "      <td> 0.040957</td>\n",
       "      <td> 0.200166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> img_00018.jpg</td>\n",
       "      <td> 0.489357</td>\n",
       "      <td> 0.054709</td>\n",
       "      <td> 0.019508</td>\n",
       "      <td> 0.010908</td>\n",
       "      <td> 0.124800</td>\n",
       "      <td> 0.079515</td>\n",
       "      <td> 0.034071</td>\n",
       "      <td> 0.187132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> img_00027.jpg</td>\n",
       "      <td> 0.379749</td>\n",
       "      <td> 0.065148</td>\n",
       "      <td> 0.038932</td>\n",
       "      <td> 0.017865</td>\n",
       "      <td> 0.117546</td>\n",
       "      <td> 0.082976</td>\n",
       "      <td> 0.044819</td>\n",
       "      <td> 0.252966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image       ALB       BET       DOL       LAG       NoF     OTHER  \\\n",
       "0  img_00005.jpg  0.367190  0.072739  0.040302  0.023501  0.130083  0.096381   \n",
       "1  img_00007.jpg  0.465186  0.056423  0.022733  0.012973  0.128789  0.083109   \n",
       "2  img_00009.jpg  0.441673  0.061966  0.026160  0.014899  0.127683  0.086496   \n",
       "3  img_00018.jpg  0.489357  0.054709  0.019508  0.010908  0.124800  0.079515   \n",
       "4  img_00027.jpg  0.379749  0.065148  0.038932  0.017865  0.117546  0.082976   \n",
       "\n",
       "      SHARK       YFT  \n",
       "0  0.053572  0.216231  \n",
       "1  0.037668  0.193119  \n",
       "2  0.040957  0.200166  \n",
       "3  0.034071  0.187132  \n",
       "4  0.044819  0.252966  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model predict\n",
    "\n",
    "test_preds1 = model.predict(test)\n",
    "\n",
    "submission = pd.DataFrame(test_preds1, columns=fish_classes)\n",
    "submission.insert(0, 'image', test_files)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
