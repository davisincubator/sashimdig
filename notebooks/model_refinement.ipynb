{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib\n",
    "import os\n",
    "from   os      import getcwd\n",
    "from   os      import listdir\n",
    "from   os.path import isfile, join, isdir\n",
    "\n",
    "import skimage\n",
    "import tensorflow as tf\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.metrics import Accuracy\n",
    "\n",
    "\n",
    "\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_paths(foldNames):\n",
    "  \n",
    "    paths = dict.fromkeys(foldNames)\n",
    "\n",
    "    for idx,g in enumerate(foldNames):\n",
    "        fileNames = [f for f in listdir(join(trainPath,g)) if isfile(join(trainPath,g, f))]\n",
    "        for i,f in enumerate(fileNames):\n",
    "            fileNames[i] = join(trainPath,g,f)     \n",
    "        paths[g] = fileNames\n",
    "        \n",
    "    return paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainPath      = '../data/raw/train'\n",
    "testPath       = '../data/raw/test_stg1'\n",
    "rawdataPath    = '../data/raw'\n",
    "\n",
    "fish_classes   = [f for f in listdir(trainPath) if isdir(join(trainPath, f))]\n",
    "groupData      = pd.DataFrame ({'group': fish_classes})\n",
    "fish_paths     = get_paths(fish_classes)\n",
    "\n",
    "subsample_amnt  = 50  # len(files) = 3777\n",
    "downsample_amnt = 2\n",
    "ROWS            = 32 # int(720  / downsample_amnt)\n",
    "COLS            = 32 # int(1280 / downsample_amnt)\n",
    "CHANNELS        = 3\n",
    "NUM_CATEGORIES  = len(fish_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `build_hdf5_image_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 1e+03 ns, total: 9 µs\n",
      "Wall time: 11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "BUILD_HDF5_DATASET = False\n",
    "IMAGE_SIZE         = 32\n",
    "VALIDATION_SPLIT   = True\n",
    "output_path        = join(rawdataPath, 'fish_dataset_{}x{}.h5'.format(IMAGE_SIZE, IMAGE_SIZE))\n",
    "input_path         = join(rawdataPath, 'train')\n",
    "\n",
    "if BUILD_HDF5_DATASET:\n",
    "    # Build a HDF5 dataset (only required once)\n",
    "    from tflearn.data_utils import build_hdf5_image_dataset\n",
    "\n",
    "\n",
    "    build_hdf5_image_dataset(target_path        =input_path, \n",
    "                             image_shape        =(IMAGE_SIZE, IMAGE_SIZE), \n",
    "                             mode               ='folder', \n",
    "                             output_path        =output_path, \n",
    "                             categorical_labels =True, \n",
    "                             normalize          =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 ms, sys: 42.4 ms, total: 57.3 ms\n",
      "Wall time: 56.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from   sklearn.model_selection    import train_test_split\n",
    "\n",
    "# Load HDF5 dataset\n",
    "import h5py\n",
    "\n",
    "h5f         = h5py.File(output_path, 'r')\n",
    "X_all       = h5f['X'][()]\n",
    "Y_all       = h5f['Y'][()]\n",
    "\n",
    "# Split into \n",
    "if VALIDATION_SPLIT:\n",
    "    X, X_valid, Y, Y_valid = train_test_split(X_all, Y_all, \n",
    "                                                          test_size    =0.2, \n",
    "                                                          random_state =23, \n",
    "                                                          stratify     =Y_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `test model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dnn_test1():\n",
    "    #needed to run this tensorflow operation in order to build the network and subsequently \n",
    "    #create the model, multiple times. Rebuilding without resetting the tf.Graph object produces\n",
    "    #errors. Could also get around this issue by restarting kernel, but that's annoying.\n",
    "    \n",
    "    #python with statement: it will use this session for the code following the with statement \n",
    "    # and then automatically close the session\n",
    "    with tf.Graph().as_default():\n",
    "#         n_nodes_hl1 = 500\n",
    "#         n_nodes_hl2 = 500\n",
    "#         n_nodes_hl3 = 500\n",
    "#         n_classes = 10\n",
    "#         batch_size = 100\n",
    "    \n",
    "        # normalisation of images\n",
    "        img_prep = ImagePreprocessing()\n",
    "        img_prep.add_featurewise_zero_center()\n",
    "        img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "        # Create extra synthetic training data by flipping & rotating images\n",
    "        img_aug = ImageAugmentation()\n",
    "        img_aug.add_random_flip_leftright()\n",
    "        img_aug.add_random_rotation(max_angle=25.)\n",
    "\n",
    "        \n",
    "        #specific a specific device or gpu\n",
    "#           with tf.device(\"/gpu:1\"): \n",
    "        \n",
    "        #input layer with shape of data specified. In this case, dimensions of our images, \n",
    "        #rows X cols X rgb array. The initial 'None' is for an unknown dimension reflecting the \n",
    "        #\"number of samples that are processed in a batch\"\n",
    "        network = input_data(shape=[None, ROWS, COLS, 3],\n",
    "                        data_preprocessing=img_prep,\n",
    "                        data_augmentation=img_aug)\n",
    "        \n",
    "        # 1: Convolution layer with 32 filters, each 3x3x3\n",
    "        conv_1 = conv_2d(network, 32, 3, activation='relu', name='conv_1')\n",
    "\n",
    "        # 2: Max pooling layer\n",
    "        network = max_pool_2d(conv_1, 2)\n",
    "\n",
    "        # 3: Convolution layer with 64 filters\n",
    "        conv_2 = conv_2d(network, 64, 3, activation='relu', name='conv_2')\n",
    "\n",
    "        # 4: Convolution layer with 64 filters\n",
    "        conv_3 = conv_2d(conv_2, 64, 3, activation='relu', name='conv_3')\n",
    "\n",
    "        # 5: Max pooling layer\n",
    "        network = max_pool_2d(conv_3, 2)\n",
    "\n",
    "        # 6: Fully-connected 512 node layer\n",
    "        network = fully_connected(network, 512, activation='relu')\n",
    "\n",
    "        # 7: Dropout layer to combat overfitting\n",
    "        network = dropout(network, 0.5)\n",
    "\n",
    "        #output latyer\n",
    "        network = fully_connected(network, 8, activation='softmax')\n",
    "        \n",
    "        # Configure how the network will be trained\n",
    "        acc = Accuracy(name=\"Accuracy\")\n",
    "        \n",
    "        network = regression(network, optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     learning_rate=0.0005, metric=acc)\n",
    "        return tflearn.DNN(network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m1.19422\u001b[0m\u001b[0m | time: 4.924s\n",
      "| Adam | epoch: 001 | loss: 1.19422 - Accuracy: 0.5567 -- iter: 3015/3021\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m1.18915\u001b[0m\u001b[0m | time: 5.989s\n",
      "| Adam | epoch: 001 | loss: 1.18915 - Accuracy: 0.5610 | val_loss: 1.11150 - val_acc: 0.6045 -- iter: 3021/3021\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Define \n",
    "\n",
    "model_tf = dnn_test1()\n",
    "\n",
    "# Start training (apply gradient descent algorithm). Will want to specify multiple epochs \n",
    "# typically unless just testing\n",
    "model_tf.fit(X, Y, n_epoch=1,validation_set=(X_valid,Y_valid),\n",
    "          show_metric=True, batch_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Train a model on the CIFAR-10 dataset and save it` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Convolutional network applied to CIFAR-10 dataset classification task.\n",
    "References:\n",
    "    Learning Multiple Layers of Features from Tiny Images, A. Krizhevsky, 2009.\n",
    "Links:\n",
    "    [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Data loading and preprocessing\n",
    "from tflearn.datasets import cifar10\n",
    "(X, Y), (X_test, Y_test) = cifar10.load_data()\n",
    "X, Y = shuffle(X, Y)\n",
    "Y = to_categorical(Y, 10)\n",
    "Y_test = to_categorical(Y_test, 10)\n",
    "\n",
    "# Real-time data preprocessing\n",
    "img_prep = ImagePreprocessing()\n",
    "img_prep.add_featurewise_zero_center()\n",
    "img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "# Real-time data augmentation\n",
    "img_aug = ImageAugmentation()\n",
    "img_aug.add_random_flip_leftright()\n",
    "img_aug.add_random_rotation(max_angle=25.)\n",
    "\n",
    "# Convolutional network building\n",
    "network = input_data(shape=[None, 32, 32, 3],\n",
    "                     data_preprocessing=img_prep,\n",
    "                     data_augmentation=img_aug)\n",
    "network = conv_2d(network, 32, 3, activation='relu')\n",
    "network = max_pool_2d(network, 2)\n",
    "network = conv_2d(network, 64, 3, activation='relu')\n",
    "network = conv_2d(network, 64, 3, activation='relu')\n",
    "network = max_pool_2d(network, 2)\n",
    "network = fully_connected(network, 512, activation='relu')\n",
    "network = dropout(network, 0.5)\n",
    "network = fully_connected(network, 10, activation='softmax')\n",
    "network = regression(network, optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     learning_rate=0.001)\n",
    "\n",
    "# Train using classifier\n",
    "model = tflearn.DNN(network, tensorboard_verbose=0,checkpoint_path='cifar10_cnn.tfl.ckpt')\n",
    "model.fit(X, Y, n_epoch=100, shuffle=True, validation_set=(X_test, Y_test),\n",
    "          show_metric=True, batch_size=96, run_id='cifar10_cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"cifar10_cnn.tfl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Refinement`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[4096,512]\n\t [[Node: FullyConnected/W/Adam_1/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@FullyConnected/W\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](FullyConnected/W/Adam_1, Adam/zeros_14)]]\n\nCaused by op u'FullyConnected/W/Adam_1/Assign', defined at:\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-35506cfaeb80>\", line 36, in <module>\n    max_checkpoints=3, tensorboard_verbose=0)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tflearn/models/dnn.py\", line 64, in __init__\n    best_val_accuracy=best_val_accuracy)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tflearn/helpers/trainer.py\", line 131, in __init__\n    clip_gradients)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tflearn/helpers/trainer.py\", line 664, in initialize_training_ops\n    name=\"apply_grad_op_\" + str(i))\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 412, in apply_gradients\n    self._create_slots(var_list)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/adam.py\", line 120, in _create_slots\n    self._zeros_slot(v, \"v\", self._name)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 656, in _zeros_slot\n    named_slots[var] = slot_creator.create_zeros_slot(var, op_name)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 123, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 101, in create_slot\n    return _create_slot_var(primary, val, '')\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 55, in _create_slot_var\n    slot = variable_scope.get_variable(scope, initializer=val, trainable=False)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 988, in get_variable\n    custom_getter=custom_getter)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 890, in get_variable\n    custom_getter=custom_getter)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 348, in get_variable\n    validate_shape=validate_shape)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 333, in _true_getter\n    caching_device=caching_device, validate_shape=validate_shape)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 684, in _get_single_variable\n    validate_shape=validate_shape)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 226, in __init__\n    expected_shape=expected_shape)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 334, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[4096,512]\n\t [[Node: FullyConnected/W/Adam_1/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@FullyConnected/W\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](FullyConnected/W/Adam_1, Adam/zeros_14)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-35506cfaeb80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m model = tflearn.DNN(regression, checkpoint_path='model_finetuning',\n\u001b[0;32m---> 36\u001b[0;31m                     max_checkpoints=3, tensorboard_verbose=0)\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;31m# Load pre-existing model, restoring all weights, except softmax layer ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cifar10_cnn.tfl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tflearn/models/dnn.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, network, clip_gradients, tensorboard_verbose, tensorboard_dir, checkpoint_path, best_checkpoint_path, max_checkpoints, session, best_val_accuracy)\u001b[0m\n\u001b[1;32m     62\u001b[0m                                \u001b[0mmax_checkpoints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_checkpoints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                                \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                                best_val_accuracy=best_val_accuracy)\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_ops, graph, clip_gradients, tensorboard_dir, tensorboard_verbose, checkpoint_path, best_checkpoint_path, max_checkpoints, keep_checkpoint_every_n_hours, random_seed, session, best_val_accuracy)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                     \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     def fit(self, feed_dicts, n_epoch=10, val_feed_dicts=None, show_metric=False,\n",
      "\u001b[0;32m/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[4096,512]\n\t [[Node: FullyConnected/W/Adam_1/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@FullyConnected/W\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](FullyConnected/W/Adam_1, Adam/zeros_14)]]\n\nCaused by op u'FullyConnected/W/Adam_1/Assign', defined at:\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-35506cfaeb80>\", line 36, in <module>\n    max_checkpoints=3, tensorboard_verbose=0)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tflearn/models/dnn.py\", line 64, in __init__\n    best_val_accuracy=best_val_accuracy)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tflearn/helpers/trainer.py\", line 131, in __init__\n    clip_gradients)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tflearn/helpers/trainer.py\", line 664, in initialize_training_ops\n    name=\"apply_grad_op_\" + str(i))\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 412, in apply_gradients\n    self._create_slots(var_list)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/adam.py\", line 120, in _create_slots\n    self._zeros_slot(v, \"v\", self._name)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 656, in _zeros_slot\n    named_slots[var] = slot_creator.create_zeros_slot(var, op_name)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 123, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 101, in create_slot\n    return _create_slot_var(primary, val, '')\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 55, in _create_slot_var\n    slot = variable_scope.get_variable(scope, initializer=val, trainable=False)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 988, in get_variable\n    custom_getter=custom_getter)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 890, in get_variable\n    custom_getter=custom_getter)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 348, in get_variable\n    validate_shape=validate_shape)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 333, in _true_getter\n    caching_device=caching_device, validate_shape=validate_shape)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 684, in _get_single_variable\n    validate_shape=validate_shape)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 226, in __init__\n    expected_shape=expected_shape)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 334, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[4096,512]\n\t [[Node: FullyConnected/W/Adam_1/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@FullyConnected/W\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](FullyConnected/W/Adam_1, Adam/zeros_14)]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Finetuning Example. Using weights from model trained in\n",
    "convnet_cifar10.py to retrain network for a new task (your own dataset).\n",
    "All weights are restored except last layer (softmax) that will be retrained\n",
    "to match the new task (finetuning).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "# Data loading\n",
    "# Note: You input here any dataset you would like to finetune\n",
    "\n",
    "\n",
    "# Redefinition of convnet_cifar10 network\n",
    "network = input_data(shape=[None, ROWS, COLS, CHANNELS])\n",
    "network = conv_2d(network, 32, 3, activation='relu')\n",
    "network = max_pool_2d(network, 2)\n",
    "network = dropout(network, 0.75)\n",
    "network = conv_2d(network, 64, 3, activation='relu')\n",
    "network = conv_2d(network, 64, 3, activation='relu')\n",
    "network = max_pool_2d(network, 2)\n",
    "network = dropout(network, 0.5)\n",
    "network = fully_connected(network, 512, activation='relu')\n",
    "network = dropout(network, 0.5)\n",
    "# Finetuning Softmax layer (Setting restore=False to not restore its weights)\n",
    "softmax = fully_connected(network, NUM_CATEGORIES, activation='softmax', restore=False)\n",
    "regression = regression(softmax, optimizer='adam',\n",
    "                        loss='categorical_crossentropy',\n",
    "                        learning_rate=0.001)\n",
    "\n",
    "model = tflearn.DNN(regression, checkpoint_path='model_finetuning',\n",
    "                    max_checkpoints=3, tensorboard_verbose=0)\n",
    "# Load pre-existing model, restoring all weights, except softmax layer ones\n",
    "model.load('cifar10_cnn.tfl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start finetuning\n",
    "model.fit(X, Y, n_epoch=1, shuffle=True,validation_set=(X_valid, Y_valid),\n",
    "          show_metric=True, batch_size=4, snapshot_step=200,\n",
    "          snapshot_epoch=False, run_id='model_finetuning')\n",
    "\n",
    "model.save('model_finetuning')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
