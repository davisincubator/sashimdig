{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib\n",
    "import os\n",
    "from   os      import getcwd\n",
    "from   os      import listdir\n",
    "from   os.path import isfile, join, isdir\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.metrics import Accuracy\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_paths(foldNames):\n",
    "  \n",
    "    paths = dict.fromkeys(foldNames)\n",
    "\n",
    "    for idx,g in enumerate(foldNames):\n",
    "        fileNames = [f for f in listdir(join(trainPath,g)) if isfile(join(trainPath,g, f))]\n",
    "        for i,f in enumerate(fileNames):\n",
    "            fileNames[i] = join(trainPath,g,f)     \n",
    "        paths[g] = fileNames\n",
    "        \n",
    "    return paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainPath      = '../data/raw/train'\n",
    "testPath       = '../data/raw/test_stg1'\n",
    "rawdataPath    = '../data/raw'\n",
    "\n",
    "fish_classes   = [f for f in listdir(trainPath) if isdir(join(trainPath, f))]\n",
    "groupData      = pd.DataFrame ({'group': fish_classes})\n",
    "fish_paths     = get_paths(fish_classes)\n",
    "\n",
    "ROWS            = 32 # int(720  / downsample_amnt)\n",
    "COLS            = 32 # int(1280 / downsample_amnt)\n",
    "CHANNELS        = 3\n",
    "NUM_CATEGORIES  = len(fish_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `build_hdf5_image_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 µs, sys: 3 µs, total: 12 µs\n",
      "Wall time: 12.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "BUILD_HDF5_DATASET = False\n",
    "IMAGE_SIZE         = 32\n",
    "VALIDATION_SPLIT   = True\n",
    "output_path        = join(rawdataPath, 'fish_dataset_{}x{}.h5'.format(IMAGE_SIZE, IMAGE_SIZE))\n",
    "input_path         = join(rawdataPath, 'train')\n",
    "\n",
    "if BUILD_HDF5_DATASET:\n",
    "    # Build a HDF5 dataset (only required once)\n",
    "    from tflearn.data_utils import build_hdf5_image_dataset\n",
    "\n",
    "\n",
    "    build_hdf5_image_dataset(target_path        =input_path, \n",
    "                             image_shape        =(IMAGE_SIZE, IMAGE_SIZE), \n",
    "                             mode               ='folder', \n",
    "                             output_path        =output_path, \n",
    "                             categorical_labels =True, \n",
    "                             normalize          =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 123 ms, sys: 51.7 ms, total: 174 ms\n",
      "Wall time: 169 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from   sklearn.model_selection    import train_test_split\n",
    "\n",
    "# Load HDF5 dataset\n",
    "import h5py\n",
    "\n",
    "h5f         = h5py.File(output_path, 'r')\n",
    "X_all       = h5f['X'][()]\n",
    "Y_all       = h5f['Y'][()]\n",
    "\n",
    "# Split into \n",
    "if VALIDATION_SPLIT:\n",
    "    X, X_valid, Y, Y_valid = train_test_split(X_all, Y_all, \n",
    "                                                          test_size    =0.2, \n",
    "                                                          random_state =23, \n",
    "                                                          stratify     =Y_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `test model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dnn_test1():\n",
    "    #needed to run this tensorflow operation in order to build the network and subsequently \n",
    "    #create the model, multiple times. Rebuilding without resetting the tf.Graph object produces\n",
    "    #errors. Could also get around this issue by restarting kernel, but that's annoying.\n",
    "    \n",
    "    #python with statement: it will use this session for the code following the with statement \n",
    "    # and then automatically close the session\n",
    "    with tf.Graph().as_default():\n",
    "#         n_nodes_hl1 = 500\n",
    "#         n_nodes_hl2 = 500\n",
    "#         n_nodes_hl3 = 500\n",
    "#         n_classes = 10\n",
    "#         batch_size = 100\n",
    "    \n",
    "        # normalisation of images\n",
    "        img_prep = ImagePreprocessing()\n",
    "        img_prep.add_featurewise_zero_center()\n",
    "        img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "        # Create extra synthetic training data by flipping & rotating images\n",
    "        img_aug = ImageAugmentation()\n",
    "        img_aug.add_random_flip_leftright()\n",
    "        img_aug.add_random_rotation(max_angle=25.)\n",
    "\n",
    "        \n",
    "        #specific a specific device or gpu\n",
    "#           with tf.device(\"/gpu:1\"): \n",
    "        \n",
    "        #input layer with shape of data specified. In this case, dimensions of our images, \n",
    "        #rows X cols X rgb array. The initial 'None' is for an unknown dimension reflecting the \n",
    "        #\"number of samples that are processed in a batch\"\n",
    "        network = input_data(shape=[None, ROWS, COLS, 3],\n",
    "                        data_preprocessing=img_prep,\n",
    "                        data_augmentation=img_aug)\n",
    "        \n",
    "        # 1: Convolution layer with 32 filters, each 3x3x3\n",
    "        conv_1 = conv_2d(network, 32, 3, activation='relu', name='conv_1')\n",
    "\n",
    "        # 2: Max pooling layer\n",
    "        network = max_pool_2d(conv_1, 2)\n",
    "\n",
    "        # 3: Convolution layer with 64 filters\n",
    "        conv_2 = conv_2d(network, 64, 3, activation='relu', name='conv_2')\n",
    "\n",
    "        # 4: Convolution layer with 64 filters\n",
    "        conv_3 = conv_2d(conv_2, 64, 3, activation='relu', name='conv_3')\n",
    "\n",
    "        # 5: Max pooling layer\n",
    "        network = max_pool_2d(conv_3, 2)\n",
    "\n",
    "        # 6: Fully-connected 512 node layer\n",
    "        network = fully_connected(network, 512, activation='relu')\n",
    "\n",
    "        # 7: Dropout layer to combat overfitting\n",
    "        network = dropout(network, 0.5)\n",
    "\n",
    "        #output latyer\n",
    "        network = fully_connected(network, 8, activation='softmax')\n",
    "        \n",
    "        # Configure how the network will be trained\n",
    "        acc = Accuracy(name=\"Accuracy\")\n",
    "        \n",
    "        network = regression(network, optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     learning_rate=0.0005, metric=acc)\n",
    "        return tflearn.DNN(network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m1.17208\u001b[0m\u001b[0m | time: 3.728s\n",
      "| Adam | epoch: 001 | loss: 1.17208 - Accuracy: 0.6038 -- iter: 3015/3021\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m1.14748\u001b[0m\u001b[0m | time: 4.792s\n",
      "| Adam | epoch: 001 | loss: 1.14748 - Accuracy: 0.6168 | val_loss: 1.13318 - val_acc: 0.6071 -- iter: 3021/3021\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Define \n",
    "\n",
    "model_tf = dnn_test1()\n",
    "\n",
    "# Start training (apply gradient descent algorithm). Will want to specify multiple epochs \n",
    "# typically unless just testing\n",
    "model_tf.fit(X, Y, n_epoch=1,validation_set=(X_valid,Y_valid),\n",
    "          show_metric=True, batch_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Train a model on the CIFAR-10 dataset and save it` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "from tflearn.datasets import cifar10\n",
    "\n",
    "(CX, CY), (CX_test, CY_test) = cifar10.load_data()\n",
    "CX, CY = shuffle(CX, CY)\n",
    "CY = to_categorical(CY, 10)\n",
    "CY_test = to_categorical(CY_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m1.27679\u001b[0m\u001b[0m | time: 19.662s\n",
      "| Adam | epoch: 001 | loss: 1.27679 - acc: 0.5541 -- iter: 49900/50000\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m1.26675\u001b[0m\u001b[0m | time: 21.106s\n",
      "| Adam | epoch: 001 | loss: 1.26675 - acc: 0.5507 | val_loss: 1.15872 - val_acc: 0.5861 -- iter: 50000/50000\n",
      "--\n",
      "INFO:tensorflow:/Users/stokesjd/repos/fish/sashimdig/notebooks/cifar10_cnn.tfl.ckpt-500 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:/Users/stokesjd/repos/fish/sashimdig/notebooks/cifar10_cnn.tfl is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "\n",
    "\n",
    "    # Real-time data preprocessing\n",
    "    img_prep = ImagePreprocessing()\n",
    "    img_prep.add_featurewise_zero_center()\n",
    "    img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "    # Real-time data augmentation\n",
    "    img_aug = ImageAugmentation()\n",
    "    img_aug.add_random_flip_leftright()\n",
    "    img_aug.add_random_rotation(max_angle=25.)\n",
    "\n",
    "    # Convolutional network building\n",
    "    network = input_data(shape=[None, 32, 32, 3],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "    network = conv_2d(network, 32, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = fully_connected(network, 512, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 10, activation='softmax')\n",
    "    network = regression(network, optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0,checkpoint_path='cifar10_cnn.tfl.ckpt')\n",
    "\n",
    "    # Train using classifier\n",
    "    model.fit(CX, CY, n_epoch=1, shuffle=True, validation_set=(CX_test, CY_test),\n",
    "              show_metric=True, batch_size=100, run_id='cifar10_cnn')\n",
    "\n",
    "    model.save(\"cifar10_cnn.tfl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 999  | total loss: \u001b[1m\u001b[32m1.05309\u001b[0m\u001b[0m | time: 19.045s\n",
      "| Adam | epoch: 001 | loss: 1.05309 - acc: 0.6317 -- iter: 49900/50000\n",
      "Training Step: 1000  | total loss: \u001b[1m\u001b[32m1.04628\u001b[0m\u001b[0m | time: 20.494s\n",
      "| Adam | epoch: 001 | loss: 1.04628 - acc: 0.6335 | val_loss: 0.93027 - val_acc: 0.6738 -- iter: 50000/50000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "\n",
    "\n",
    "    # Real-time data preprocessing\n",
    "    img_prep = ImagePreprocessing()\n",
    "    img_prep.add_featurewise_zero_center()\n",
    "    img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "    # Real-time data augmentation\n",
    "    img_aug = ImageAugmentation()\n",
    "    img_aug.add_random_flip_leftright()\n",
    "    img_aug.add_random_rotation(max_angle=25.)\n",
    "\n",
    "    # Convolutional network building\n",
    "    network = input_data(shape=[None, 32, 32, 3],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "    network = conv_2d(network, 32, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = fully_connected(network, 512, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 10, activation='softmax')\n",
    "    network = regression(network, optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    model_ref = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model_ref.load('cifar10_cnn.tfl')\n",
    "    # Train using classifier\n",
    "    model_ref.fit(CX, CY, n_epoch=1, shuffle=True, validation_set=(CX_test, CY_test),\n",
    "              show_metric=True, batch_size=100, run_id='cifar10_cnn_refine')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Assign requires shapes of both tensors to match. lhs shape= [8] rhs shape= [10]\n\t [[Node: save_1/Assign_39 = Assign[T=DT_FLOAT, _class=[\"loc:@FullyConnected_1/b\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](FullyConnected_1/b/Adam_1, save_1/RestoreV2_39/_9)]]\n\nCaused by op u'save_1/Assign_39', defined at:\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-2bc95804e7f6>\", line 29, in <module>\n    model_ref = tflearn.DNN(network, tensorboard_verbose=0)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tflearn/models/dnn.py\", line 64, in __init__\n    best_val_accuracy=best_val_accuracy)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tflearn/helpers/trainer.py\", line 145, in __init__\n    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\n    self.build()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1081, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 414, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [8] rhs shape= [10]\n\t [[Node: save_1/Assign_39 = Assign[T=DT_FLOAT, _class=[\"loc:@FullyConnected_1/b\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](FullyConnected_1/b/Adam_1, save_1/RestoreV2_39/_9)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2bc95804e7f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                          learning_rate=0.001)\n\u001b[1;32m     29\u001b[0m     \u001b[0mmodel_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_verbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mmodel_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cifar10_cnn.tfl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Train using classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     model_ref.fit(X, Y, n_epoch=1, shuffle=True, validation_set=(X_valid, Y_valid),\n",
      "\u001b[0;32m/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tflearn/models/dnn.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, model_file, weights_only, **optargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m                      \u001b[0mcreated\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrestored\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \"\"\"\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         self.predictor = Evaluator([self.net],\n",
      "\u001b[0;32m/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, model_file, trainable_variable_only, variable_name_map, scope_for_restore, create_new_session, verbose)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mrestorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrainable_variable_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestorer_trainvars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1437\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1439\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [8] rhs shape= [10]\n\t [[Node: save_1/Assign_39 = Assign[T=DT_FLOAT, _class=[\"loc:@FullyConnected_1/b\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](FullyConnected_1/b/Adam_1, save_1/RestoreV2_39/_9)]]\n\nCaused by op u'save_1/Assign_39', defined at:\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-2bc95804e7f6>\", line 29, in <module>\n    model_ref = tflearn.DNN(network, tensorboard_verbose=0)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tflearn/models/dnn.py\", line 64, in __init__\n    best_val_accuracy=best_val_accuracy)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tflearn/helpers/trainer.py\", line 145, in __init__\n    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\n    self.build()\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1081, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 414, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/stokesjd/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [8] rhs shape= [10]\n\t [[Node: save_1/Assign_39 = Assign[T=DT_FLOAT, _class=[\"loc:@FullyConnected_1/b\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](FullyConnected_1/b/Adam_1, save_1/RestoreV2_39/_9)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "\n",
    "\n",
    "    # Real-time data preprocessing\n",
    "    img_prep = ImagePreprocessing()\n",
    "    img_prep.add_featurewise_zero_center()\n",
    "    img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "    # Real-time data augmentation\n",
    "    img_aug = ImageAugmentation()\n",
    "    img_aug.add_random_flip_leftright()\n",
    "    img_aug.add_random_rotation(max_angle=25.)\n",
    "\n",
    "    # Convolutional network building\n",
    "    network = input_data(shape=[None, 32, 32, 3],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "    network = conv_2d(network, 32, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = fully_connected(network, 512, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, NUM_CATEGORIES, activation='softmax',restore='False')\n",
    "    network = regression(network, optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    model_ref = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model_ref.load('cifar10_cnn.tfl')\n",
    "    # Train using classifier\n",
    "    model_ref.fit(X, Y, n_epoch=1, shuffle=True, validation_set=(X_valid, Y_valid),\n",
    "              show_metric=True, batch_size=100, run_id='fish_cnn_refine')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = tflearn.DNN(regression, checkpoint_path='model_finetuning',\n",
    "#                     max_checkpoints=3, tensorboard_verbose=0)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
