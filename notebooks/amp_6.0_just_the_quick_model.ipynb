{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import tflearn packages\n",
    "\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tflearn.data_utils import shuffle\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "\n",
    "#import other packages\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import os\n",
    "from os import getcwd\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "\n",
    "import skimage\n",
    "from skimage import measure\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#things to set\n",
    "trainPath = '../data/processed/gray'\n",
    "testPath = '../data/processed/test/gray'\n",
    "outname = 'submission_170225_gray.csv'\n",
    "\n",
    "##need to adjust channels somehow because it's black and white now!\n",
    "\n",
    "ROWS = 90  #90 720\n",
    "COLS = 160 #160 1280\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import images\n",
    "\n",
    "def get_paths(foldNames):\n",
    "  \n",
    "    paths = dict.fromkeys(foldNames)\n",
    "\n",
    "    for idx,g in enumerate(foldNames):\n",
    "        fileNames = [f for f in listdir(join(trainPath,g)) if isfile(join(trainPath,g, f))]\n",
    "        for i,f in enumerate(fileNames):\n",
    "            fileNames[i] = join(trainPath,g,f)     \n",
    "        paths[g] = fileNames\n",
    "        \n",
    "    return paths\n",
    "\n",
    "fish_classes = [f for f in listdir(trainPath) if isdir(join(trainPath, f))]\n",
    "groupData = pd.DataFrame ({'group': fish_classes})\n",
    "fish_paths = get_paths(fish_classes)\n",
    "\n",
    "#remove mac added files\n",
    "keys_to_remove = [key for key, value in fish_paths.iteritems()\n",
    "                  if '.DS_Store' in value]\n",
    "\n",
    "for key in keys_to_remove:\n",
    "    del fish_paths[key]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#label images by directory\n",
    "\n",
    "for idx,fish in enumerate(fish_classes):\n",
    "    groupData.ix[idx,'num files'] = int(len(fish_paths[fish]))\n",
    "    \n",
    "files = []\n",
    "Y_cat = []\n",
    "\n",
    "for fish in fish_classes:\n",
    "    fish_files = fish_paths[fish]\n",
    "    files.extend(fish_files)\n",
    "    \n",
    "    y_fish = np.tile(fish, len(fish_files))\n",
    "    Y_cat.extend(y_fish)\n",
    "\n",
    "\n",
    "#remove mac added files\n",
    "is_to_remove = [i for i in range(0,len(files))\n",
    "                  if '.DS_Store' in files[i]]\n",
    "\n",
    "for i in is_to_remove:\n",
    "    del files[i]\n",
    "    del Y_cat[i]\n",
    "        \n",
    "#change to numpy array\n",
    "Y_cat = np.array(Y_cat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#downsample images\n",
    "def read_image(src):\n",
    "    \"\"\"Read and resize individual images\"\"\"\n",
    "    im = io.imread(src)\n",
    "    im = resize(im, (ROWS, COLS))\n",
    "    return im\n",
    "\n",
    "#change to numpy array\n",
    "X_all = np.ndarray((len(files), ROWS, COLS, CHANNELS), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (90,160,4) into shape (90,160,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-13141d03264d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mX_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processed {} of {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (90,160,4) into shape (90,160,1)"
     ]
    }
   ],
   "source": [
    "for i, f in enumerate(files): \n",
    "    im = read_image(f)\n",
    "    X_all[i] = im\n",
    "    if i%1000 == 0: print('Processed {} of {}'.format(i, len(files)))\n",
    "\n",
    "##view example image\n",
    "image = X_all[0]\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(image, cmap='gray', interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split into train and test for cross-validation\n",
    "# One Hot Encoding Labels\n",
    "#    Transform the categorical array Y_all into matrix of the same height, \n",
    "#    but with a boolean column for each category.\n",
    "Y_all = LabelEncoder().fit_transform(Y_cat)\n",
    "Y_all = np_utils.to_categorical(Y_all)\n",
    "\n",
    "# test_size: between 0 and 1. proportion of the dataset to include in the test split\n",
    "# random_state: Pseudo-random number generator state used for random sampling. How to shoose this?\n",
    "# stratify: this is ensuring that the split datasets are balanced, i.e. contains the same \n",
    "# percentage of classes\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_all, Y_all, \n",
    "                                                    test_size=0.2, random_state=23, \n",
    "                                                    stratify=Y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in test photo set now so it's there regardless of if I do dnn1 or dnn2\n",
    "test_files = [im for im in os.listdir(testPath)]\n",
    "test = np.ndarray((len(test_files), ROWS, COLS, CHANNELS), dtype=float)\n",
    "for i, im in enumerate(test_files): \n",
    "    test[i] = read_image(join(testPath,im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build a simple CNN\n",
    "\n",
    "def dnn_test1():\n",
    "    #needed to run this tensorflow operation in order to build the network and subsequently \n",
    "    #create the model, multiple times. Rebuilding without resetting the tf.Graph object produces\n",
    "    #errors. Could also get around this issue by restarting kernel, but that's annoying.\n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        #input layer with shape of data specified. In this case, dimensions of our images, \n",
    "        #rows X cols X rgb array. The initial 'None' is for an unknown dimension reflecting the \n",
    "        #\"number of samples that are processed in a batch\"\n",
    "        \n",
    "        # Building convolutional network\n",
    "\n",
    "        net = input_data(shape=[None, ROWS, COLS, 3])\n",
    "        net = conv_2d(net, 32, 3, activation='relu', regularizer=\"L2\")\n",
    "        net = max_pool_2d(net, 2)\n",
    "        net = local_response_normalization(net)\n",
    "        net = conv_2d(net, 64, 3, activation='relu', regularizer=\"L2\")\n",
    "        net = max_pool_2d(net, 2)\n",
    "        net = local_response_normalization(net)\n",
    "        net = fully_connected(net, 72, activation='relu')\n",
    "        net = fully_connected(net, 8, activation='softmax')\n",
    "        net = regression(net)\n",
    "        return tflearn.DNN(net)\n",
    "\n",
    "#test on subset of training images you didn't use; if it works well could do a full train:test and submit\n",
    "# Define model\n",
    "model = dnn_test1()\n",
    "\n",
    "# Start training (apply gradient descent algorithm). Will want to specify multiple epochs \n",
    "# typically unless just testing\n",
    "model.fit(X_train, Y_train, n_epoch=10,\n",
    "          show_metric=True, batch_size=16, validation_set = (X_valid, Y_valid))\n",
    "\n",
    "#model predict\n",
    "\n",
    "test_preds1 = model.predict(test)\n",
    "\n",
    "submission = pd.DataFrame(test_preds1, columns=fish_classes)\n",
    "submission.insert(0, 'image', test_files)\n",
    "submission.head()\n",
    "\n",
    "submission.to_csv('../data/processed/tflearn/' + outname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
